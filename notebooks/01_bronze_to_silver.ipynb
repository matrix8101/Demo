{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c20bc4-c48b-4657-bbba-f40b3e054673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utils/common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4072a67b-3b3e-478e-8db9-a8510ffc0ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this to see the true workspace path\n",
    "print(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50565227-e23e-4e2e-9004-904b53f3ae89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load the centralized configuration\n",
    "config = get_pipeline_config()\n",
    "\n",
    "# 2. Authenticate to ADLS (Direct Access)\n",
    "authenticate_adls(config['storage_account'], config['secret_scope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21974c02-7631-4976-861e-b0e9e0066ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a widget to allow ADF to specify a specific file, \n",
    "# or leave it as \"ALL\" to process the whole folder.\n",
    "dbutils.widgets.text(\"input_file_name\", \"ALL\")\n",
    "file_param = dbutils.widgets.get(\"input_file_name\")\n",
    "\n",
    "input_container = config['container_input']\n",
    "storage_account = config['storage_account']\n",
    "\n",
    "# Base path for the input container\n",
    "base_input_path = f\"abfss://{input_container}@{storage_account}.dfs.core.windows.net/\"\n",
    "\n",
    "# Logic to determine which files to process\n",
    "if file_param.upper() == \"ALL\":\n",
    "    # List all files in the input folder\n",
    "    files_to_process = [f.name for f in dbutils.fs.ls(base_input_path) if f.name.endswith('.csv')]\n",
    "else:\n",
    "    files_to_process = [file_param]\n",
    "\n",
    "print(f\"Files identified for processing: {files_to_process}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa66ac0-8ad9-4f9e-9c29-e91cde4f4285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "import re\n",
    "\n",
    "for file in files_to_process:\n",
    "    try:\n",
    "        print(f\"--- Starting processing for: {file} ---\")\n",
    "        \n",
    "        # 1. Build Paths\n",
    "        current_input_path = f\"{base_input_path}{file}\"\n",
    "        table_name = file.replace(\".csv\", \"\").replace(\" \", \"_\").lower()\n",
    "        silver_path = get_path(config['container_output'], storage_account, f\"silver/{table_name}\")\n",
    "        \n",
    "        # 2. Read with SEMICOLON delimiter\n",
    "        df_raw = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"sep\", \";\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(current_input_path)\n",
    "        \n",
    "        # 3. Clean Column Names (Best Practice)\n",
    "        # Removes quotes, spaces, and semicolons from column headers\n",
    "        for col in df_raw.columns:\n",
    "            clean_col = re.sub(r'[ ,;{}()\\n\\t=]', '_', col).replace('\"', '')\n",
    "            df_raw = df_raw.withColumnRenamed(col, clean_col)\n",
    "        \n",
    "        # 4. Add Audit Metadata\n",
    "        df_silver = df_raw.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "                          .withColumn(\"source_filename\", input_file_name())\n",
    "        \n",
    "        # 5. Write as Delta\n",
    "        df_silver.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .save(silver_path)\n",
    "            \n",
    "        print(f\"✅ Successfully processed {file} into Silver table: {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to process {file}. Error: {str(e)}\")\n",
    "\n",
    "print(\"--- All files processed ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_to_silver",
   "widgets": {
    "input_file_name": {
     "currentValue": "ALL",
     "nuid": "cbc2004c-a263-46dd-9f6e-b00a0930aae0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ALL",
      "label": null,
      "name": "input_file_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ALL",
      "label": null,
      "name": "input_file_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
