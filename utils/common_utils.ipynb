{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d800f8ff-c99c-4900-b02d-035423f86f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def get_pipeline_config():\n",
    "    \"\"\"Reads the JSON config file from the repo.\"\"\"\n",
    "    # This gets the directory where THIS notebook (common_utils) is located\n",
    "    current_dir = os.getcwd() \n",
    "    \n",
    "    # We go up one level from 'utils' to 'Demo', then into 'config'\n",
    "    config_path = os.path.abspath(os.path.join(current_dir, \"..\", \"config\", \"env_config.json\"))\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found at {config_path}. Check your folder structure!\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def authenticate_adls(storage_account, scope):\n",
    "    \"\"\"Sets configurations for direct ADLS access using Hadoop Spark Context.\"\"\"\n",
    "    account_url = f\"{storage_account}.dfs.core.windows.net\"\n",
    "    \n",
    "    # Get the underlying Hadoop Configuration\n",
    "    # This is more robust than spark.conf.set in Shared/UC clusters\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    \n",
    "    # Set the Auth Type\n",
    "    hconf.set(f\"fs.azure.account.auth.type.{account_url}\", \"OAuth\")\n",
    "    hconf.set(f\"fs.azure.account.oauth.provider.type.{account_url}\", \n",
    "              \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    \n",
    "    # Set the Credentials from Secrets\n",
    "    hconf.set(f\"fs.azure.account.oauth2.client.id.{account_url}\", dbutils.secrets.get(scope, \"client-id\"))\n",
    "    hconf.set(f\"fs.azure.account.oauth2.client.secret.{account_url}\", dbutils.secrets.get(scope, \"client-secret\"))\n",
    "    hconf.set(f\"fs.azure.account.oauth2.client.endpoint.{account_url}\", \n",
    "              f\"https://login.microsoftonline.com/{dbutils.secrets.get(scope, 'tenant-id')}/oauth2/token\")\n",
    "    \n",
    "    print(f\"âœ… Hadoop Auth configurations set for {storage_account}\")\n",
    "\n",
    "def get_path(container, storage_account, relative_path):\n",
    "    \"\"\"Constructs a full ABFSS path.\"\"\"\n",
    "    # Ensure no double slashes if relative_path starts with one\n",
    "    relative_path = relative_path.lstrip('/')\n",
    "    return f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{relative_path}\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
